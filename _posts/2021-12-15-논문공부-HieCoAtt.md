---
layout: post
author: 한지상
title: "Paper study - HieCoAtt"
date: 2021-12-15 12:10:00
categories: Papers_Study
tags: [논문, VQA, Vision]
use_math: true
---

<img src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fonground-korea.github.io%2Fpapers_study%2F2021%2F12%2F15%2F%25EB%2585%25BC%25EB%25AC%25B8%25EA%25B3%25B5%25EB%25B6%2580-HieCoAtt.html&count_bg=%232E49A0&title_bg=%23555555&icon=&icon_color=%23E7E7E7&title=hits&edge_flat=true" width="6%">

# HieCoAtt

Conference: Neurips2016  
Presenter: 한지상  
Tags: 5주차  
Title: Hierarchical Question-Image Co-Attention for Visual Question   Answering  
URL: https://arxiv.org/pdf/1606.00061.pdf  


## Abstract

기존의 VQA 모델들에서는 질문과 관련된 이미지에 영역을 표시함.

이 논문에서는 이미지 뿐만 아니라 "`what words to listen to`" 즉, 텍스트에도 같은 중요성이 있다고 판단함.

→ 계층적 구조로 질문을 학습히고, 텍스트에도 Attention을 넣음.

## 1. Introduction

**Co-Attention**과 **Question Hierarchy** 의 특징을 가진 multi-modal attention model을 제시함.

- **Co-Attention** : 이미지와 질문이 서로를 유도하면서 대칭적인 관계를 가지게 함.
- **Question Hierarchy** : (a)단어 (b)구문 (c)질문(문장전체) 의 세가지 레벨로 구조화하여 계층화 함.

![/assets/HieCoAtt/Untitled.png](/assets/HieCoAtt/Untitled.png)

단어, 구문, 질문 레벨의 임베딩을 추출하고, 각 레벨에서 이미지와 질문에 co-attention 을 적용함. 최종 정답예측은 모든 co-attended 이미지와 질문 특성에 기반함.

**단어층**에서는 이미지에서 각 물체가 무엇이 있는지

**구문층**에서는 그것들 사이의 관계를

**질문층**에서는 이미지에 던진 질문을 본다.

## 2. Related Work

## 3. Method

### 3.1 Notation

**질문** : T 개의 단어로, $Q=\{q_1,... q_T\}$ 로 나타냄. $q_t$는 t번째 단어의 feature vector.

$q_t^w, q_t^p, q_t^s$ 는 각각 word embedding, phrase embedding, question embedding.

**이미지 특성** : $V=\{v_1,... v_N\}$ $v_n$은 위치n에서의 feature vector.

**이미지와 질문의 co-attention feature** : $\hat{v}^r, \hat{q}^r , r\in \{w, p, s\}$ , 모든 weight들은 W로 표기.

### 3.2 Question Hierarchy

- 원-핫 인코딩 된 단어들 $Q=\{q_1,... q_T\}$을 벡터 공간에 임베딩시켜 $Q^w=\{q_1^w,...,q_T^w\}$를 얻음.

- 구문 feature을 계산하기 위해 1-D convolution을 단어 임베딩 벡터에 적용한다.
    
    구문은 여러 길이의 단어들로 구성되므로 → 각각 단어의 위치에서 1, 2, 3-gram으로  (세 가지 윈도우 사이즈의 필터) 단어벡터의 내적을 계산함. 
    
    ![t번째 단어에서 윈도우사이즈 s에서의 convolution output. W는 문장에서의 weight.](/assets/HieCoAtt/Untitled1.png)
    
    t번째 단어에서 윈도우사이즈 s에서의 convolution output. W는 문장에서의 weight.
    

- 각 n-gram을 1-D conv에 넣을 때 0-padded를 해주어 모두 길이가 같게 만듦. 그리고 각 t에서 maxpooling을 하여 구문 벡터를 얻는다.
    
    ![구문 벡터](/assets/HieCoAtt/Untitled2.png)
    
    구문 벡터
    

- 질문 벡터는 구문 벡터를 LSTM으로 인코딩한 것의 hidden vector을 사용.
    
    질문 feature $q_t^s$ 은 시간 t에서 LSTM hidden vector.
    
    ![/assets/HieCoAtt/Untitled%203.png](/assets/HieCoAtt/Untitled3.png)
    

### 3.3 Co-Attention

- **Parallel Co-Attention**
    
    두 가지 feature에 대한 attention을 동시에 잡음. → 이미지와 질문을 합친 affinity matrix C 를 만듦. 
    
    이미지 위치와 질문 위치의 모든 쌍에서 이미지와 질문 특징 간의 유사성을 계산하여 이미지와 질문을 연결.
    
    ![/assets/HieCoAtt/Untitled%204.png](/assets/HieCoAtt/Untitled4.png)
    
    이미지 feature map $V \in R^{d\times N}$, 질문 representation $Q\in R^{d \times T}$, affinity matrix $C\in R^{T\times N}$
    
    C의 row는 질문 attention space를 이미지 attention space로 보내는 행렬. C의 transpose는 그와 반대.
    
    affinity matrix를 계산한 후, 이미지(또는 질문) attention을 계산하기 위해 레이어를 만들고 max값을 계산.
    
    ![/assets/HieCoAtt/Untitled%205.png](/assets/HieCoAtt/Untitled5.png)
    
    $a^v, a^q$는 attention probabilities. (각각 이미지 region $v_n$, 단어 $q_t$)
    
    ![/assets/HieCoAtt/Untitled%206.png](/assets/HieCoAtt/Untitled6.png)
    
    이미지와 질문의 features의 weighted sum을 계산하여 attention vector을 계산함.
    
    ![/assets/HieCoAtt/Untitled%207.png](/assets/HieCoAtt/Untitled7.png)
    
    ![정답을 도출하는 모델./???](/assets/HieCoAtt/Untitled8.png)
    
    정답을 도출하는 모델./???
    
    ![/assets/HieCoAtt/Untitled%209.png](/assets/HieCoAtt/Untitled9.png)
    
    단어 레벨의 attention을 fc에 넣어 단어 레벨의 hidden layer을 만들고, 구문 레벨의 attention을 concat하여 fc에 넣고, 문장 레벨의 hidden layer에서 softmax를 사용하여 정답을 도출.
    

## 4. Experiment

[제목 없음](/assets/HieCoAtt/%E1%84%8C%E1%85%A6%E1%84%86%E1%85%A9%E1%86%A8%20%E1%84%8B%E1%85%A5%E1%86%B9%E1%84%82%E1%85%B3%E1%86%AB%20%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%E1%84%87%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%89%E1%85%B3%202fa3790705454af887a90c8abf0bc7fd.csv)

### 4.3 Results and Analysis

![/assets/HieCoAtt/Untitled%2010.png](/assets/HieCoAtt/Untitled10.png)

![/assets/HieCoAtt/Untitled%2011.png](/assets/HieCoAtt/Untitled11.png)

### 4.5 Qualitative Results

![/assets/HieCoAtt/Untitled%2012.png](/assets/HieCoAtt/Untitled12.png)

논문의 모델은 단어 레벨에서 특히 객체에 집중함.

첫 번째, 두 번째 이미지에서는 단어 레벨에서 구문 레벨로 갈 때 세 번째 이미지와 달리 사물에서 배경으로 주의가 이동함. 이는 질문의 유형이 다름으로부터 나오는 결과라고 예측하고 있음.

## 5. Conclusion

본 논문에서는 VQA를 위한 계층적 co-attention모델을 제안하였음.

co-attention은 서로 다른 이미지 영역과 부분에 주의를 기울일 수 있게 하였음.