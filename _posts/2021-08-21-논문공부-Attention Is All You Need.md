---
layout: post
author: í•œì§€ìƒ
title: "Paper study - Attention Is All You Need"
date: 2021-08-21 02:58:00
categories: Papers Study
tags: [ë…¼ë¬¸, VQA, Vision]
use_math: true
---

# Transformer

Conference: Neurpis2017  
Presenter: í•œì§€ìƒ  
Title: Attention Is All You Need  
URL: https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf
  
## Abstract

- ì£¼ëœ sequence transduction modelsëŠ” encoderì™€ decoderì„ í¬í•¨í•œ ë³µì¡í•œ recurrent ë˜ëŠ” convolutional neural networkì— ê¸°ë°˜í•œë‹¤.

**Transformer** - attention mechanism ì—ë§Œ ê¸°ë°˜. 

- ì´ ëª¨ë¸ì€ WMT 2014 ì˜ì–´â†’ë…ì¼ì–´ ë²ˆì—­ taskì—ì„œ ì´ì „ë³´ë‹¤ 2 ë†’ì€ 28.4 BLEUë¥¼ ë‹¬ì„±í•˜ì˜€ë‹¤. ì—¬ê¸°ì„œ ì´ ëª¨ë¸ì€ 8ê°œì˜ GPUë¡œ 8ì¼ ë™ì•ˆ í•™ìŠµì‹œì¼œ 41.8ì ì˜ BLEU state-of-the-art ë‹¨ì¼ ëª¨ë¸ì´ë‹¤.

ì´ ë…¼ë¬¸ì—ì„œ **Transformer**ëŠ” í¬ê±°ë‚˜ í•œì •ëœ í•™ìŠµ ë°ì´í„°ë¥¼ ê°€ì§€ê³ ì„œë„ ì„±ê³µì ìœ¼ë¡œ ë‹¤ë¥¸ taskë“¤ì— ì¼ë°˜í™”ë  ìˆ˜ ìˆìŒì„ ë³´ì¸ë‹¤.

## 1. Introduction

RNN, LSTM, GRUëŠ” sequence ëª¨ë¸ë§ ë¬¸ì œì—ì„œ ë›°ì–´ë‚œ ì„±ê³¼ë¥¼ ë³´ì´ë©° í™•ê³ íˆ ìë¦¬ë¥¼ ì¡ì•˜ë‹¤. 

í•˜ì§€ë§Œ recurrentëª¨ë¸ì€  

- í•™ìŠµë˜ëŠ” ë¬¸ì¥ì´ ê¸¸ì–´ì§ˆìˆ˜ë¡ ì„œë¡œ ë©€ë¦¬ ë–¨ì–´ì§„ ë¬¸ì¥ì— ëŒ€í•œ ì •ë³´ê°€ ì¤„ì–´ë“¤ì–´ ì œëŒ€ë¡œëœ ì˜ˆì¸¡ì„ í•  ìˆ˜ ì—†ë‹¤. â†’ **Long-term dependency problem**
- ìˆœì°¨ì ì¸ ì—°ì‚°ìœ¼ë¡œ ë³‘ë ¬í™”ê°€ ë¶ˆê°€ëŠ¥í•˜ì—¬ ì—°ì‚°ì†ë„ê°€ ì €í•˜ëœë‹¤.

â†’ Recurrentëª¨ë¸ì˜ ì œì•½ì‚¬í•­ë“¤ì„ í”¼í•˜ê³  ì…ì¶œë ¥ ì‚¬ì´ì— ì „ì—­ ì˜ì¡´ì„±ì„ ì´ëŒì–´ë‚´ê¸° ìœ„í•´ **Transformer**ì„ ì œì•ˆí•œë‹¤. (ë³‘ë ¬í™”ê°€ ê°€ëŠ¥í•˜ë‹¤.)

## 2. Background

## 3. Model Architecture

![https://greeksharifa.github.io/public/img/2019-08-17-Attention%20Is%20All%20You%20Need/01.png](https://greeksharifa.github.io/public/img/2019-08-17-Attention%20Is%20All%20You%20Need/01.png)

Transformer - encoder / decoder 

- encoder - symbol representations (ì…ë ¥)$(x_1, ... , x_n)$ ì„ continuous representations $z=(z_1, ... , z_n)$ìœ¼ë¡œ ë§¤í•‘í•œë‹¤.
- decoder - zê°€ ì£¼ì–´ì§€ë©´, ë””ì½”ë”ëŠ” í•œë²ˆì— í•œ ì›ì†Œì”© ì¶œë ¥ sequence $(y_1, ... , y_n)$ë¥¼ ìƒì„±í•œë‹¤.
- ê° ë‹¨ê³„ëŠ” auto-regressive (ìë™íšŒê·€)ì´ë©°, ë‹¤ìŒ ë‹¨ê³„ì˜ symbolì„ ìƒì„±í•  ë•Œ ì´ì „ ë‹¨ê³„ì—ì„œ ìƒì„±ëœ symbolì„ ì¶”ê°€ ì…ë ¥ìœ¼ë¡œ ë°›ëŠ”ë‹¤.

### Encoder

- N = 6ê°œì˜ ë™ì¼í•œ layerë¡œ êµ¬ì„±ëœë‹¤. ì²˜ìŒ inputì´ ì²« ë²ˆì§¸ layerì— ë“¤ì–´ê°€ê³  ë‹¤ìŒ layerì€ ì´ì „ layerì˜ ê²°ê³¼ê°’ì´ ë“¤ì–´ê°„ë‹¤. $layer(x)$ê°€ ë‹¤ì‹œ $layer$ì— ë“¤ì–´ê°.
- ê° layerì€ multi-head self-attention mechanism ê³¼ simple, position-wise fully connected feed-forward network ë¡œ êµ¬ì„±ëœë‹¤.
- ê° sub-layerì˜ ì¶œë ¥ê°’ - $LayerNorm(x+Sublayer(x))$
- $Sublayer(x)$ëŠ” sub-layer ìì²´ë¡œ êµ¬í˜„ë˜ëŠ” í•¨ìˆ˜ì´ë‹¤.
- ì´ëŸ¬í•œ residual connectionì„ ìš©ì´í•˜ê²Œ í•˜ê¸° ìœ„í•´ embedding layerì„ í¬í•¨í•œ ëª¨ë“  sub-layerì€ $d_{model}=512$ ì°¨ì›ì˜ ì¶œë ¥ê°’ì„ ê°€ì§„ë‹¤.

### Decoder

- N = 6ê°œì˜ ë™ì¼í•œ layerë¡œ êµ¬ì„±ëœë‹¤.
- sub-layerëŠ” Encoderì˜ ê²ƒê³¼ ë™ì¼í•˜ì§€ë§Œ Encoder stackì˜ ì¶œë ¥ê°’ì— multi-head attentionì„ ìˆ˜í–‰í•˜ëŠ” sub-layerì„ ë” ê°€ì§„ë‹¤.
- Decoderê°€ ì¶œë ¥ì„ ìƒì„±í•  ë•Œ ë‹¤ìŒ ì¶œë ¥ì—ì„œ ì •ë³´ë¥¼ ì–»ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´ maskingì„ ì‚¬ìš©í•œë‹¤. (ì¹˜íŒ…ë°©ì§€ ğŸ¤¥)

## 3.2 Attention

Attentioní•¨ìˆ˜ëŠ” `query + key-value -> output` ìœ¼ë¡œì˜ ë³€í™˜ì„ ìˆ˜í–‰í•œë‹¤. 

query, key-value, output: ë²¡í„°

outputì€ valueë“¤ì˜ ê°€ì¤‘í•©ìœ¼ë¡œ ê³„ì‚°ë˜ë©°, ê°€ì¤‘ì¹˜ëŠ” queryì™€ ì—°ê´€ëœ keyì˜ í˜¸í™˜ì„± í•¨ìˆ˜(compatibility function)ì— ì˜í•´ ê³„ì‚°ëœë‹¤.

### 3.2.1 Scaled Dot-Product Attention

![/assets/Transformer/Untitled.png](/assets/Transformer/Untitled.png)

- Input - $d_k$ì°¨ì›ì˜ queryì™€ key, $d_v$ì°¨ì›ì˜ value

![](/assets/Transformer/Untitled1.png)


queryì™€ ëª¨ë“  keyì˜ dot-productë¥¼ ê³„ì‚°í•˜ê³ , ê°ê° $\sqrt{d_k}$ë¡œ ë‚˜ëˆ„ê³ (scaling), valueì˜ ê°€ì¤‘ì¹˜ë¥¼ ì–»ê¸° ìœ„í•´ softmaxí•¨ìˆ˜ë¥¼ ì ìš©í•œë‹¤.

â†’ queryì™€ ìœ ì‚¬í•œ valueì¼ìˆ˜ë¡, ë” ë†’ì€ ê°’ì„ ê°€ì§. **Attention**

queryë“¤ì— ëŒ€í•´ ë™ì‹œì— ê³„ì‚°í•˜ê¸° ìœ„í•´ ì´ë“¤ì„ í–‰ë ¬ Që¡œ ë¬¶ëŠ”ë‹¤. key - K, value - V

Q: ë””ì½”ë”ì˜ ì´ì „ layer hidden state

K: ì¸ì½”ë”ì˜ output state

V: ì¸ì½”ë”ì˜ output state

### 3.2.2 Multi-Head Attention

![/assets/Transformer/Untitled%202.png](/assets/Transformer/Untitled2.png)

$d_{model}$ì°¨ì›ì˜ query, key, valueë¡œ ë‹¨ì¼ attention functionì„ ì‚¬ìš©í•˜ëŠ” ê²ƒë³´ë‹¤ ì´ë“¤ì„ ê°ê° $d_k, d_k, d_v$ì°¨ì›ìœ¼ë¡œ ê°ê° ë‹¤ë¥´ê²Œ $h$ë²ˆ í•™ìŠµì‹œí‚¤ëŠ” ê²ƒì´ ë‚«ë‹¤. 

â†’ ê° sub-layerì— ë™ì¼í•œ ë¶€ë¶„ì´ hê°œ ì¡´ì¬í•œë‹¤ëŠ” ëœ».

ê°ê° ê³„ì‚°ëœ $h$ìŒì˜ $d_v$ì°¨ì›ì˜ ì¶œë ¥ì„ concatenateí•œ í›„ ì„ í˜•í•¨ìˆ˜ì— í†µê³¼ì‹œì¼œ ìµœì¢… ì¶œë ¥ê°’ì„ ê³„ì‚°í•œë‹¤.

![/assets/Transformer/Untitled%203.png](/assets/Transformer/Untitled3.png)

![/assets/Transformer/Untitled%204.png](/assets/Transformer/Untitled4.png)

ë…¼ë¬¸ì—ì„œëŠ” $h=8, d_k=d_v=d_{model}/h=64$ë¥¼ ì‚¬ìš©í•˜ì˜€ë‹¤.

### 3.2.3 Applications of Attention in out Model

- encoder-decoder Attention layerì—ì„œ queryëŠ” ì´ì „ë””ì½”ë”ì˜ layerì—ì„œ, keyì™€ valueëŠ” ì¸ì½”ë”ì˜ ì¶œë ¥ì—ì„œ ì˜¨ë‹¤. â†’ ë””ì½”ë”ê°€ ì…ë ¥ì˜ ëª¨ë“  ìœ„ì¹˜(ì›ì†Œ)ë¥¼ ê³ ë ¤í•  ìˆ˜ ìˆë„ë¡ í•œë‹¤.
- ì¸ì½”ë”ëŠ” self-attention layerì„ í¬í•¨í•œë‹¤. â†’ ì¸ì½”ë”ì˜ ê° ì›ì†ŒëŠ” ì´ì „ layerì˜ ëª¨ë“  ì›ì†Œë¥¼ ê³ ë ¤í•  ìˆ˜ ìˆë‹¤.
- ë””ì½”ë”ì—ì„œëŠ” maskingì„ í†µí•´ ë¯¸ë˜ì‹œì ì˜ ë‹¨ì–´ë“¤ì„ ë¯¸ë¦¬ ì¡°íšŒí•¨ì— ë”°ë¼ í˜„ì¬ë‹¨ì–´ ê²°ì •ì— ë¯¸ì¹  ìˆ˜ ìˆëŠ” ì˜í–¥ì„ ë§‰ëŠ”ë‹¤. ì´ë¥¼ ìœ„í•´  ië²ˆì§¸ positionì— ëŒ€í•œ attentionì„ ì–»ì„ ë•Œ , ië²ˆì§¸ ì´í›„ì— ìˆëŠ” ëª¨ë“  positionì€ Attentionì‹ì—ì„œ softmaxì˜ inputê°’ì„ $-inf$ì— ê°€ê¹Œìš´ ë§¤ìš° ì‘ì€ ìˆ˜ë¡œ ì„¤ì •í•œë‹¤. â†’ ië²ˆì§¸ ì´í›„ì— ìˆëŠ” positionì— attentionì„ ì£¼ëŠ” ê²½ìš°ê°€ ì—†ê² ì£µ

## 3.3 Position-wise Feed-Forward Networks

$FFN(x)=max(0,xW_1+b_1)W_2+b_2$

Linear Transformation â†’ ReLU â†’ Linear Transformation ë¡œ ì´ë£¨ì–´ì ¸ìˆë‹¤.

## 3.5 Positional Encoding

Transformerì—ì„œëŠ” recurrenceì™€ convolutionì„ ì‚¬ìš©í•˜ì§€ ì•Šê¸°ë•Œë¬¸ì— ë‹¨ì–´ì˜ sequenceë¥¼ ì´ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” ë‹¨ì–´ì˜ positionì— ëŒ€í•œ ì •ë³´ê°€ í•„ìš”í•˜ë‹¤.

â†’ ì¸ì½”ë”ì™€ ë””ì½”ë”ì˜ input embeddingì— positional encodingì„ ë”í•´ì¤€ë‹¤.

- positional encodingì€ $d_{model}$(embedding)ê³¼ ê°™ì€ ì°¨ì›ì„ ê°–ëŠ”ë‹¤.

ëª¨ë¸ì—ì„œëŠ” sin, cosí•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì˜€ë‹¤.

![/assets/Transformer/Untitled%205.png](/assets/Transformer/Untitled5.png)

- pos: position | i: dimension | ì£¼ê¸°: $10000^{2i/d_{model}}2\pi$
- pos - sequenceì—ì„œ ë‹¨ì–´ì˜ ìœ„ì¹˜, í•´ë‹¹ ë‹¨ì–´ëŠ” $i : 0 -> d_{model}/2$

# 5. Training

[](/assets/Transformer/%E1%84%8C%E1%85%A6%E1%84%86%E1%85%A9%E1%86%A8%20%E1%84%8B%E1%85%A5%E1%86%B9%E1%84%82%E1%85%B3%E1%86%AB%20%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%E1%84%87%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%89%E1%85%B3%200303f15617e0400896b5a81ce6273cc5.csv)

- Optimizer

Adam optimizerì—ì„œ learning rateë¥¼ ê³ ì •ì‹œí‚¤ì§€ ì•Šê³  ë³€í™”ì‹œí‚´.

![/assets/Transformer/Untitled%206.png](/assets/Transformer/Untitled6.png)

warmup_stepê¹Œì§€ëŠ” ì„ í˜•ì ìœ¼ë¡œ ì¦ê°€í•˜ë‹¤ê°€ ì´í›„ì—ëŠ” step_numì˜ inverse square rootì— ë¹„ë¡€í•˜ë„ë¡ ê°ì†Œì‹œí‚¨ë‹¤. 

ì²˜ìŒì—ëŠ” í•™ìŠµì´ ì˜ ë˜ì§€ ì•Šì€ ìƒíƒœì´ë¯€ë¡œ learning rateë¥¼ ë¹ ë¥´ê²Œ ì¦ê°€ì‹œì¼œ ë³€í™”ë¥¼ í¬ê²Œ ì£¼ë‹¤ê°€ í•™ìŠµì´ ì–´ëŠì •ë„ ë  ë•Œ, ë³€í™”ë¥¼ ì‘ê²Œ ì£¼ê¸° ìœ„í•¨.

# 6. Results

![/assets/Transformer/Untitled%207.png](/assets/Transformer/Untitled7.png)

![/assets/Transformer/Untitled%208.png](/assets/Transformer/Untitled8.png)

![/assets/Transformer/Untitled%209.png](/assets/Transformer/Untitled9.png)

# 7. Conclusion

encoderì™€ decoderì—ì„œ attentionì„ í†µí•´ queryì™€ ê°€ì¥ ë°€ì ‘í•œ ì—°ê´€ì„±ì„ ê°€ì§€ëŠ” valueë¥¼ ê°•ì¡°í•  ìˆ˜ ìˆê³  ë³‘ë ¬í™”ê°€ ê°€ëŠ¥í•˜ë‹¹...